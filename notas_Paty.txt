27-05-2023 00:58
estudiar
- Batch normalización
- Dropout
- Por qué se separa en train test y eval:
Train y eval son para calcular la loss por época, buscar cuál es mejor. Test se usa para la evaluación final.

tips
- Scheduler al entrenar y evaluar modelos por épocas, sirve para mejorar learning rate, sirve para más?

----
Cosas para el tutorial:
El get_batch que usa bptt  en verdad es nuestro dataloader que le ingresa un batch_size, ej 32.
Para el vocabulario: posibilidad de usar binning para la tasa de disparo, así el vocabulario se reduce.
Actualmente tengo baks y los valores de la matriz es gigante, ya que tiene flotantes largos, otra solución es transformar esos valores aproximandolos,
 así igual está la posibilidad de reducir el vocabulario.

Al procesar los SUA para definir el vocabulario, guardar
vocab, nro ocurrencias
¿ definir stop words ?, el cero es stopword ?

Lo que entra a Transformers:
Ntokens= largo del vocabulario
Emsize= definir el tamaño que quiero como embedding, el embedding lo calcula con una función genérica pytorch
 en time2vector lo definimos en 64 (valor arbitrario)

----
en resumen lo que debo hacer:
- calcular el vocabulario, usando baks round o binning (listo)
- definir dataset train, eval, test
    sera necesario separar en 5 splits?, para empezar NO
    esto es debajo de la función generate_vocabulary
- usar DataLoader para transformers
- del tutorial falta traspasar desde la linea 264 en adelante.
(hasta aca listo)

- cambiar el criterio loss a MSE, se cae.
- cambiar el 116 fijo por un max que abarque a todos los archivos 
rellenando con 0 o truncando.
- probar con Adam.
- probar con otro archivo.
- guardar pesos de transformers en un directorio NO temporal.

--------------------
Para la loss:
https://neptune.ai/blog/pytorch-loss-functions
- Ocupare una para regresión, ya que mis datos son continuos.

- Para converger más rápido probar estandarizando los datos con: 
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

- Para separar en train y test se puede ocupar esto: 
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=2)

--------------------
- En model de transformers, el input realmente no son los datos, 
sino que el índice del dato en el vocabulario.
- Dado que el vocabulario se armo a partir de los datos aproximados a un decimal, y a la vez se genero un 
archivo con los mismos datos aproximados al decimal, entonces para obtener el índice de estos datos 
en el dataset simplemente se multiplican todos los datos de X por 10**decimal.

