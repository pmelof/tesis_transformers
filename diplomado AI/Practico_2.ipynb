{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Practico 2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHopPtVaNF1K",
        "colab_type": "text"
      },
      "source": [
        "# **Diplomado IA: Inteligencia Artificial I - Parte 2**. <br> Práctico 2: Funciones de Pérdida, Regularización y Tareas Auxiliares\n",
        "---\n",
        "---\n",
        "\n",
        "**Profesores:**\n",
        "- Carlos Aspillaga\n",
        "- Gabriel Sepúlveda\n",
        "\n",
        "**Ayudante:**\n",
        "- Andrés Carvallo\n",
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4--58EgLXxI",
        "colab_type": "text"
      },
      "source": [
        "# **Instrucciones Generales**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmhnKlt8Ns7A",
        "colab_type": "text"
      },
      "source": [
        "El siguiente práctico se puede realizar **individualmente o en parejas**. Solo uno debe realizar la entrega. El formato de entrega es el **archivo .ipynb con todas las celdas ejecutadas**. Todas las preguntas deben ser respondidas en las celdas dispuestas para ello. No se aceptará el _output_ de una celda de código como respuesta.\n",
        "\n",
        "**Nombre compañero 1:** COMPLETAR\n",
        "\n",
        "**Nombre compañero 2:** COMPLETAR\n",
        "\n",
        "El siguiente práctico cuanta con 3 experimentos donde cada uno contendrá actividades a realizar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCb8075QLdVx",
        "colab_type": "text"
      },
      "source": [
        "# **Índice**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGlX5q_pQYRa",
        "colab_type": "toc"
      },
      "source": [
        ">[Diplomado IA: Inteligencia Artificial I - Parte 2.  Práctico 2: Funciones de Pérdida, Regularización y Tareas Auxiliares](#scrollTo=tHopPtVaNF1K)\n",
        "\n",
        ">[Instrucciones Generales](#scrollTo=D4--58EgLXxI)\n",
        "\n",
        ">[Índice](#scrollTo=jCb8075QLdVx)\n",
        "\n",
        ">[Funciones de Pérdida](#scrollTo=QowbUszVAyrv)\n",
        "\n",
        ">>[Experimento 1](#scrollTo=9BqupBJjCJvp)\n",
        "\n",
        ">>>[Entrenamiento de un clasificador con función de pérdida Cross-Entropy](#scrollTo=QUjcMm1-1Q68)\n",
        "\n",
        ">>>[Entrenamiento de un clasificador con función de pérdida MSE](#scrollTo=Fqc4VRj62wp4)\n",
        "\n",
        ">>>[Actividades](#scrollTo=QiOarh-8d81k)\n",
        "\n",
        ">[Regularización](#scrollTo=XmRz5CaMDuNJ)\n",
        "\n",
        ">>[Experimento 2](#scrollTo=Zo4Vn12EBCjW)\n",
        "\n",
        ">>>[Entrenamiento de un modelo sin regularización](#scrollTo=ePn_NE89yWY9)\n",
        "\n",
        ">>>[¿ Qué pasa si agregamos regularización L2 ?](#scrollTo=e91eKoCmWEWG)\n",
        "\n",
        ">>>[Actividades](#scrollTo=rKftFZ2W4XKJ)\n",
        "\n",
        ">[Tareas auxiliares](#scrollTo=O5CrhP8kEOnu)\n",
        "\n",
        ">>[Experimento 3](#scrollTo=8FVLGUPjBy9l)\n",
        "\n",
        ">>[BONUS Opcional: 5 ganadores](#scrollTo=csAEszF6CHdT)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QowbUszVAyrv",
        "colab_type": "text"
      },
      "source": [
        "# **Funciones de Pérdida**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BqupBJjCJvp",
        "colab_type": "text"
      },
      "source": [
        "## Experimento 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUjcMm1-1Q68",
        "colab_type": "text"
      },
      "source": [
        "### Entrenamiento de un clasificador con función de pérdida Cross-Entropy\n",
        "\n",
        "En el siguiente experimento vamos a utilizar el dataset _CIFAR-10_, que contiene imágenes de 10 clases (Avión, Auto, Pájaro, Gato, Ciervo, Perro Rana, Caballo, Barco y Camión). El objetivo será entrenar un clasificador capaz de identificar la clase solamente observando imagen."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PShwkZ5Rrg2O",
        "colab_type": "text"
      },
      "source": [
        "Primero crearemos nuestro Dataloader, que se encarga de manejar los datos de entrada al modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUyrqiheCVAY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "# Download Datasets provided by pytorch\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "\n",
        "# Create Dataloaders\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=128,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "# List class names, for analysis purposes\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "# Create a dictionary of DataLoaders for train y test\n",
        "loaders = { 'train': trainloader, 'test': testloader }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRXT1P2RrtaT",
        "colab_type": "text"
      },
      "source": [
        "Ahora visualizaremos algunas imágenes de muestra"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2J_xbTpdpwk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pylab\n",
        "\n",
        "%matplotlib inline\n",
        "pylab.rcParams['figure.figsize']=18,10\n",
        "\n",
        "# functions to show an image\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# get 7 random training images\n",
        "num_images = 7\n",
        "images, labels = iter(trainloader).next()\n",
        "images = images[:num_images,:,:,:]\n",
        "labels = labels[:num_images]\n",
        "\n",
        "# show images and labels\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "print(' '*7+''.join(str(classes[labels[i]]).ljust(17) for i in range(num_images)).strip())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WJ9EZySGyvg",
        "colab_type": "text"
      },
      "source": [
        "A continuación creamos nuestro modelo para clasificar imágenes. \n",
        "\n",
        "En este caso, nuestro modelo tendrá 2 capas convolucionales y 3 capas densas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUg8HJt91DFS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!if [ ! -f modelo1.png ]; then wget -q https://www.dropbox.com/s/6z6nedx6sjkz757/modelo1.png; fi\n",
        "import IPython \n",
        "pylab.rcParams['figure.figsize']=10,10\n",
        "pil_img = IPython.display.Image(filename='modelo1.png')\n",
        "display(pil_img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPlrpRpRd5JQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, output_dim=10):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-bLtzkoHZnj",
        "colab_type": "text"
      },
      "source": [
        "Aquí crearemos una función para mostrar la matriz de confusión"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftyd24lN4ISh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import itertools\n",
        "import numpy as np\n",
        "\n",
        "def print_conf_matrix(predictions, targets, classes, normalize=False, cmap=plt.cm.Blues):\n",
        "    stacked = torch.stack((targets, predictions), dim=1)\n",
        "    cm = torch.zeros(len(classes), len(classes), dtype=torch.int64)\n",
        "    for p in stacked:\n",
        "      tl, pl = p.tolist()\n",
        "      cm[tl, pl] += 1\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        title = \"Normalized confusion matrix\"\n",
        "    else:\n",
        "        title = 'Confusion matrix, without normalization'\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwV9-LDRHinI",
        "colab_type": "text"
      },
      "source": [
        "Aqui se implementa el loop de entrenamiento y evaluacion en el set de validación"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQhP3bZTwZvE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_test_model(model, loader, loss_function, n_epochs, classes):\n",
        "    # Create the loss function object\n",
        "    if loss_function == 'MSE':\n",
        "        loss_fn = nn.MSELoss()\n",
        "    else:\n",
        "        loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Create the optimizer\n",
        "    optimizer= torch.optim.SGD(model.parameters(),lr=0.01, momentum=0.9)\n",
        "\n",
        "    # This is our training loop\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        print(\"Epoch N° {}\".format(epoch))\n",
        "\n",
        "        for phase in ['train','test']:\n",
        "            if phase == 'train':\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "                \n",
        "            cum_loss = 0.0\n",
        "            cum_acc = 0.0\n",
        "            cum_cnt = 0\n",
        "            phase_preds = []\n",
        "            phase_targets = []\n",
        "            for n_batch, (sample, target) in enumerate(loaders[phase]):\n",
        "                \n",
        "                if loss_function == 'MSE':\n",
        "                    # Format change so that target is consistent with the prediction format\n",
        "                    target = target.float().unsqueeze(1)\n",
        "\n",
        "                if phase == 'train': \n",
        "                    # During training, we reset gradients at every batch iteration\n",
        "                    optimizer.zero_grad()\n",
        "                \n",
        "                # Run model (forward pass)\n",
        "                result = model(sample)\n",
        "\n",
        "                # Compute loss\n",
        "                loss = loss_fn(result, target)\n",
        "\n",
        "                cum_loss += loss.item()\n",
        "                if loss_function == 'MSE':\n",
        "                    # Round predictions to the nearest integer in [0,9]\n",
        "                    preds = torch.clamp(torch.round(result), 0, 9)\n",
        "                else:\n",
        "                    # Prediction in the greatest value in a one-hot encoding vector\n",
        "                    _, preds = torch.max(result,1) \n",
        "\n",
        "                if phase == 'train':\n",
        "                    # During training, we also do a backward pass and do a gradient step\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                else:\n",
        "                    # Store the values needed to compute the confusion matrix\n",
        "                    phase_preds.append(preds)\n",
        "                    phase_targets.append(target)\n",
        "\n",
        "                cum_acc += torch.sum((preds == target).float())\n",
        "                cum_cnt += target.size(0)\n",
        "                print(\"\\r{}-Batch {}/{} Loss: {:.4f} Acc: {:.2f}%\".format(phase.upper(),\n",
        "                                                                              n_batch+1,\n",
        "                                                                              len(loaders[phase]),\n",
        "                                                                              cum_loss/cum_cnt,\n",
        "                                                                              100*cum_acc/cum_cnt), end=\"\")\n",
        "            if phase == 'test':\n",
        "                # print the confusion matrix for the entire epoch\n",
        "                phase_preds = torch.cat(phase_preds, dim=0)\n",
        "                phase_targets = torch.cat(phase_targets, dim=0)\n",
        "                print_conf_matrix(phase_preds, phase_targets, classes)\n",
        "\n",
        "            print(\"\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GWlCG5pJeYR",
        "colab_type": "text"
      },
      "source": [
        "Ahora entrenaremos nuestro modelo usando Cross-Entropy.\n",
        "\n",
        "Recordemos la fórmula de la Cross-Entropy: \n",
        "\n",
        "$$-\\sum_{x_i,y_i \\in T}{y_i * \\log{q(x_i)}} = -\\sum_{x_i,y_i \\in T}{y_i * \\log{\\hat{y}_i}}$$\n",
        "\n",
        "Suponiendo que el modelo retorna un vector $[q_1$ .. $q_{10}]^T$, para un ejemplo de clase 3 nos quedará un término de este estilo (existira uno así para cada ejemplo de entrenamiento):\n",
        "\n",
        "$$ \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}^T \\begin{pmatrix} \\log{q_1} \\\\ \\log{q_2} \\\\ \\log{q_3} \\\\ \\log{q_4} \\\\ \\log{q_5} \\\\ \\log{q_6} \\\\ \\log{q_7} \\\\ \\log{q_8} \\\\ \\log{q_9} \\\\ \\log{q_{10}} \\end{pmatrix} = \\log{q_3}$$\n",
        "\n",
        "Con esto vemos que el único valor que nos importa maximizar es $q_3$, lo cual es ideal porque dejamos libre la distribución de los errores. Todos los otros $q_j$ solo nos importan de manera agregada (pues $\\sum_j{q_j} = 1$)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNwmEVCteXha",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "pylab.rcParams['figure.figsize']=10,10\n",
        "\n",
        "# Código para asegurar que el random parta del mismo punto al momento de ejecutar el modelo\n",
        "np.random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed(0)\n",
        "\n",
        "# Instanciar el modelo\n",
        "model = Net(output_dim=len(classes))\n",
        "\n",
        "# Ejecutar función para entrenar el modelo y testear.                  \n",
        "train_test_model(model, loaders, 'Cross-Entropy', 2, classes)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fqc4VRj62wp4",
        "colab_type": "text"
      },
      "source": [
        "### Entrenamiento de un clasificador con función de pérdida MSE\n",
        "\n",
        "Ahora estudiemos la MSE. Veremos por qué razón no es buena idea utilizarla en un problema de clasificacion.\n",
        "\n",
        "Usaremos el mismo modelo anterior, con la única diferencis de que esta vez usaremos una salida de 1 dimensión, que usaremos para identificar la clase (valor de 0 a 9)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55Co0mUO3Rwm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!if [ ! -f modelo2.png ]; then wget -q https://www.dropbox.com/s/680xmwsfdzg71nh/modelo2.png; fi\n",
        "import IPython \n",
        "pil_img = IPython.display.Image(filename='modelo2.png')\n",
        "display(pil_img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "602rCM_VTW-U",
        "colab_type": "text"
      },
      "source": [
        "Recordemos la fórmula de la MSE asumiendo que el modelo retorna una predicción como un único valor entre 0 y 9 para indicar la clase: \n",
        "\n",
        "$$\\frac{1}{N} \\sum_{x_i,y_i \\in T}^N{(\\hat{y}_{i} - y_{i})^2}$$\n",
        "\n",
        "Es fácil ver que si el modelo predijera siempre clase 9, entonces tendría el siguiente MSE (asumiendo clases balanceadas):\n",
        "\n",
        "$$\\frac{(9-0)^2 + (9-1)^2 + (9-2)^2 + (9-3)^2 + (9-4)^2 + (9-5)^2 + (9-6)^2 + (9-7)^2 + (9-8)^2 +(9-9)^2}{10} = $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iw8F1Xijas36",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(9**2 + 8**2 + 7**2 + 6**2 + 5**2 + 4**2 + 3**2 + 2**2 +1**2 + 0**2)/10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V43F9y5jbLTm",
        "colab_type": "text"
      },
      "source": [
        "En cambio, si el modelo predijera siempre clase 5, tendría el siguiente MSE (asumiendo clases balanceadas):\n",
        "\n",
        "$$\\frac{(5-0)^2 + (5-1)^2 + (5-2)^2 + (5-3)^2 + (5-4)^2 + (5-5)^2 + (5-6)^2 + (5-7)^2 + (5-8)^2 +(5-9)^2}{10} = $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qf_BP3OgbbAw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(5**2 + 4**2 + 3**2 + 2**2 + 1**2 + 0**2 + (-1)**2 + (-2)**2 + (-3)**2 + (-4)**2)/10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tj7S35p2b0cV",
        "colab_type": "text"
      },
      "source": [
        "Con esto vemos que el modelo no es indiferente al valor numérico que le asigna a las predicciones, pues errores grandes se penalizan más que errores pequeños.\n",
        "\n",
        "Ahora repetiremos el mismo entrenamiento que hicimos con Cross-Entropy, pero esta vez usando pérdida MSE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MV-amcAQ61Qo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Código para asegurar que el random parta del mismo punto al momento de ejecutar el modelo\n",
        "np.random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed(0)\n",
        "\n",
        "# Instanciar el modelo\n",
        "model = Net(output_dim=1)\n",
        "\n",
        "# Ejecutar función para entrenar el modelo y testear.                  \n",
        "train_test_model(model, loaders, 'MSE', 10, classes)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QiOarh-8d81k",
        "colab_type": "text"
      },
      "source": [
        "### Actividades"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XjuCylRd_h0",
        "colab_type": "text"
      },
      "source": [
        "**¿En cuáles de estos problemas usaría MSE en lugar de Cross-Entropy?\n",
        "Marque el check-box de todas las alternativas que correspondan**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUc7GuscgSjJ",
        "colab_type": "text"
      },
      "source": [
        "*   **1) Un sistema que permite decidir si un comentario en un foro muestra una opinión positiva o negativa**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIs6pojzepXT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Respuesta 1\n",
        "Respuesta = False #@param {type:\"boolean\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYu1-GWZghLS",
        "colab_type": "text"
      },
      "source": [
        "*   **2) Un sistema que a partir del historial de compras de un cliente, estima la probabilidad de que sea hombre o mujer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cN1qQxQgwzA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Respuesta 2\n",
        "Respuesta = False #@param {type:\"boolean\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zxnvin-1iWxn",
        "colab_type": "text"
      },
      "source": [
        "*   **3) Un sistema que a estima el precio del Bitcoin a partir del ánalisis de los mensajes de twitter relacionados**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2l04jCliwXF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Respuesta 3\n",
        "Respuesta = False #@param {type:\"boolean\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxQedd5Bi5Rf",
        "colab_type": "text"
      },
      "source": [
        "*   **4) Un sistema que a debe aproximar una función continua altamente no lineal**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeuOPy3Ijccm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Respuesta 4\n",
        "Respuesta = False #@param {type:\"boolean\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMnev_zujeuh",
        "colab_type": "text"
      },
      "source": [
        "*   **5) Un sistema que a debe aproximar una función discreta que mide la cantidad de alumnos que asistirán a la próxima clase (un número entero entre 1 y 40), a partir del título de la clase y el nombre del profesor**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTmakNt4jhcM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Respuesta 5\n",
        "Respuesta = False #@param {type:\"boolean\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3ckTe0tlOTh",
        "colab_type": "text"
      },
      "source": [
        "*   **6) Un sistema que estima el color dominante de un objeto a partir de su imagen en blanco y negro (mediante la predicción de 3 valores: R: Cantidad de Rojo, G: Cantidad de Verde, B: Cantidad de Azul.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kn_td02Gjk4z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Respuesta 6\n",
        "Respuesta = False #@param {type:\"boolean\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmRz5CaMDuNJ",
        "colab_type": "text"
      },
      "source": [
        "# **Regularización**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zo4Vn12EBCjW",
        "colab_type": "text"
      },
      "source": [
        "## Experimento 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePn_NE89yWY9",
        "colab_type": "text"
      },
      "source": [
        "### Entrenamiento de un modelo sin regularización\n",
        "\n",
        "A continuación analizaremos el efecto de aplicar regularización a la función de pérdida al entrenar una red neuronal para una tarea de clasificación.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDeZgHNN9ORQ",
        "colab_type": "text"
      },
      "source": [
        "El código que aparece a continuación permite la descarga de un archivo que contiene el dataset que utilizaremos en la presente sección.\n",
        "Este archivo contiene un conjunto de vectores bidimensionales, los que pueden pertencer a una de las dos clases definidas: 0 o 1. El archivo consta de 4 columnas separadas por coma, las  cuales corresponden a:\n",
        " * Coordenada x del punto\n",
        " * Coordenada y del punto\n",
        " * Identificador de la clase (0 o 1)\n",
        " * Identificador de si pertenece al conjunto de entrenamiento (1) o test (0)\n",
        "\n",
        "Luego de descargar el archivo, se imprimirán las primeras y las últimas 5 líneas del archivo para que observe su contenido."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUZWs-NJy0BT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!if [ ! -f sample_data.csv ]; then wget -q https://www.dropbox.com/s/amibk8qtgvtw7bk/sample_data.csv; fi\n",
        "!head -5 sample_data.csv\n",
        "!echo \"----------------------------------------\"\n",
        "!tail -5 sample_data.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qXiMWhc_d6O",
        "colab_type": "text"
      },
      "source": [
        "El siguiente código define una función que permitirá leer los datos desde el archivo descargado, y una función para poder graficar los datos asignando colores a cada vector según su clase.\n",
        "Posteriormente se hace un llamado a cada una de ellas para visualizar como se distribuyen las clases."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44tsqEN_y5JP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pylab\n",
        "\n",
        "%matplotlib inline\n",
        "pylab.rcParams['figure.figsize']=10,8\n",
        "\n",
        "def load_data( filename ):\n",
        "  data_train = {'class_1': [], 'class_2': [], 'raw': []}\n",
        "  data_test = {'class_1': [], 'class_2': [], 'raw': []}\n",
        "  with open(filename, newline='') as csvfile:\n",
        "    datareader = csv.reader(csvfile)\n",
        "    for row in datareader:\n",
        "      if row[0] == 'X1': # skip header\n",
        "        continue\n",
        "      if int(row[3]) == 1: # training sample\n",
        "        data_train['raw'].append([float(row[0]), float(row[1]), float(row[2])])\n",
        "        if float(row[2]) == 0.0:\n",
        "          data_train['class_1'].append([float(row[0]), float(row[1])])\n",
        "        else:\n",
        "          data_train['class_2'].append([float(row[0]), float(row[1])])\n",
        "      else: # test sample\n",
        "        data_test['raw'].append([float(row[0]), float(row[1]), float(row[2])])\n",
        "        if float(row[2]) == 0.0:\n",
        "          data_test['class_1'].append([float(row[0]), float(row[1])])\n",
        "        else:\n",
        "          data_test['class_2'].append([float(row[0]), float(row[1])])\n",
        "  data_train = { key: np.array(data_train[key]) for key in data_train }\n",
        "  data_test = { key: np.array(data_test[key]) for key in data_test }\n",
        "  return data_train, data_test\n",
        "\n",
        "def plot_dataset(data, title):\n",
        "  plt.plot(data['class_1'][:,0], data['class_1'][:,1], 'o', label=r'class 1')\n",
        "  plt.plot(data['class_2'][:,0], data['class_2'][:,1], 'o', label=r'class 2')\n",
        "  plt.title(title, fontsize=17)\n",
        "  plt.ylabel('y', fontsize=14)\n",
        "  plt.xlabel('x', fontsize=14)\n",
        "  plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0., fontsize=25)\n",
        "  plt.grid()\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyXeXeOlzJYQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_train, data_test = load_data('sample_data.csv')\n",
        "plot_dataset(data_train, 'Conjunto de Datos de Entrenamiento')\n",
        "plot_dataset(data_test, 'Conjunto de Datos de Test')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5oalscYAR4k",
        "colab_type": "text"
      },
      "source": [
        "A continuación se definen las clases necesarias para definir un *Dataloader*, el cual es un objeto que permitirá iterar cómodamente sobre los datos dentro del loop de entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rel4aQaTzNY7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class TwoClassesDataset(Dataset):\n",
        "\n",
        "  def __init__(self, dataset, transform = None):\n",
        "    self.dataset = dataset\n",
        "    self.transform = transform\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.dataset)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    if torch.is_tensor( idx):\n",
        "      idx = idx.tolist()\n",
        "    sample = {'input': self.dataset[idx, :2], 'target': self.dataset[idx, 2]}\n",
        "    if self.transform:\n",
        "      sample = self.transform(sample)\n",
        "    return sample\n",
        "\n",
        "# Conversor de datos generados por dataset\n",
        "class ToTensor(object):\n",
        "  def __call__(self, sample):\n",
        "    input_, target_ = sample['input'].astype(np.float32), int(sample['target'])\n",
        "    return {'input': torch.from_numpy(input_), 'target': target_ }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQ-q4o7Q2zid",
        "colab_type": "text"
      },
      "source": [
        "La siguiente clase, que llamaremos ClassificationModel, implementa una red neuronal de 3 capas utilizando el framework *PyTorch*. Esta recibe como entreda un valor de dos dimensiones (x, y) y su estructura interna ha sido definida como:\n",
        "\n",
        " * Layer 1: 1000 unidades\n",
        " * Layer 2: 1000 unidades\n",
        " * Layer 3 (salida): 2 unidades"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kO5hNI1K1wWe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class ClassificationModel( nn.Module ):\n",
        "\n",
        "  def __init__( self ):\n",
        "    super(ClassificationModel, self).__init__()\n",
        "    self.actfunc_relu = nn.ReLU()\n",
        "    self.layer1 = nn.Linear(2, 1000) # Capa 1: 2 entradas y 1000 salidas\n",
        "    self.layer2 = nn.Linear(1000, 1000) # Capa 2: 1000 entradas y 1000 salidas\n",
        "    self.layer3 = nn.Linear(1000, 1) # Capa 3: 1000 entradas y 1 salida\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.actfunc_relu(self.layer1(x))\n",
        "    x = self.actfunc_relu(self.layer2(x))\n",
        "    x = self.layer3(x)\n",
        "    # notar que no usamos una softmax pues usaremos una loss que la trae incorporada\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mURxoZGL3mQv",
        "colab_type": "text"
      },
      "source": [
        "La siguiente función implementa el loop de entrenamiento que se encargará de actualizar los parámetros de la red con el fin de ajustar el modelo a los datos de entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYktN3o31zJD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "def fit(model, criterion, optimizer, dataloader, n_epochs = 100):\n",
        "  scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.9, patience=10, verbose=False)\n",
        "  for epoch in range(n_epochs):\n",
        "    model.train()\n",
        "    epoch_train_loss = 0.0\n",
        "    epoch_train_accu = 0.0\n",
        "    epoch_tot_elements = 0.0\n",
        "    for i_batch, sample_batched in enumerate(dataloader):\n",
        "      input_batch = sample_batched['input'].to(device)\n",
        "      target_batch = sample_batched['target'].to(device).unsqueeze(1).float()\n",
        "\n",
        "      prediction = model(input_batch)\n",
        "      loss = criterion(prediction, target_batch)\n",
        "      accuracy = model_accuracy(prediction, target_batch)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      batch_sz = float(input_batch.size(0))\n",
        "      epoch_train_loss += loss.item() * batch_sz\n",
        "      epoch_train_accu += accuracy * batch_sz\n",
        "      epoch_tot_elements += batch_sz\n",
        "\n",
        "    epoch_train_loss /= epoch_tot_elements\n",
        "    epoch_train_accu /= epoch_tot_elements\n",
        "\n",
        "    # Decay Learning Rate\n",
        "    scheduler.step(epoch_train_accu)\n",
        "\n",
        "    print('Epoch %d: train loss %f acc %f' % (epoch+1, epoch_train_loss, epoch_train_accu))\n",
        "\n",
        "def evaluate(model, criterion, dataloader):\n",
        "  model.eval()\n",
        "  test_loss = 0.0\n",
        "  test_accu = 0.0\n",
        "  tot_elements = 0\n",
        "  for i_batch, sample_batched in enumerate(dataloader):\n",
        "    input_batch = sample_batched['input'].to(device)\n",
        "    target_batch = sample_batched['target'].to(device).unsqueeze(1).float()\n",
        "\n",
        "    prediction = model(input_batch)\n",
        "    loss = criterion(prediction, target_batch)\n",
        "    accuracy = model_accuracy(prediction, target_batch)\n",
        "\n",
        "    batch_sz = float(input_batch.size(0))\n",
        "    test_loss += loss.item() * batch_sz\n",
        "    test_accu += accuracy * batch_sz\n",
        "    tot_elements += batch_sz\n",
        "\n",
        "  test_loss /= tot_elements\n",
        "  test_accu /= tot_elements\n",
        "  print('Test loss %f acc %f' % (test_loss, test_accu))\n",
        "\n",
        "def model_accuracy(predicted, target):\n",
        "  correct = ((predicted > 0.0) == (target > 0.0)).float().sum()\n",
        "  accuracy = float(correct) / float(target.shape[0])\n",
        "  return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DgGu3Lp3t0A",
        "colab_type": "text"
      },
      "source": [
        "A continuación se crean los objetos encargados de:\n",
        "\n",
        " * Dataloader: encargado de cargar los datos e iterar sobre ellos\n",
        " * Modelo de clasificación: clase ClassificationModel\n",
        " * Función de pérdida: CrossEntropyLoss\n",
        " * Algoritmo de optimización: Adam\n",
        "\n",
        "Dentro del algoritmo de optimización podrá encontrar el parámetro *weight_decay*. Este permite controlar el nivel de penalización aplicado sobre la función de pérdida a través de una regularización del tipo L2.\n",
        "\n",
        "Finalmente, se inicia el entrenamiento del modelo durante 100 épocas. Este es realizado por medio de un llamado a la función *fit* definida más arriba."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBUdhefy11Rk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Creación de Dataloader\n",
        "two_classes_trainset = TwoClassesDataset(data_train['raw'], transform = transforms.Compose([ToTensor()]))\n",
        "dataloader_train = DataLoader(two_classes_trainset, batch_size = 256, shuffle = True, num_workers = 1)\n",
        "two_classes_testset = TwoClassesDataset(data_test['raw'], transform = transforms.Compose([ToTensor()]))\n",
        "dataloader_test = DataLoader(two_classes_testset, batch_size = 256, shuffle = False, num_workers = 1)\n",
        "\n",
        "# Creación de modelo\n",
        "model = ClassificationModel()\n",
        "model.to(device)\n",
        "\n",
        "# Creación de instancias para la función de pérdida y optimizador\n",
        "# Parámetro Weight Decay indica la importancia que asignaremos a la regularización L2 en la pérdida (en este caso, 0)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr = 1e-3, weight_decay = 0)\n",
        "\n",
        "# Entrenamiento del modelo\n",
        "fit(model, criterion, optimizer, dataloader_train, 150)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBXNrM2072En",
        "colab_type": "text"
      },
      "source": [
        "A continuación revisaremos cómo le fue al modelo en el set de test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozAbRLdu7_pF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "evaluate(model, criterion, dataloader_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2muXS8whDUmk",
        "colab_type": "text"
      },
      "source": [
        "La siguiente función permite crear un gráfico con el límite de clasificación aprendido por el modelo, y los datos de entrenamiento entregados durante entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X24ZSdIX16IR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pylab\n",
        "\n",
        "%matplotlib inline\n",
        "pylab.rcParams['figure.figsize']=10,8\n",
        "\n",
        "def plot_classification_surface(model, dataset, title):\n",
        "  x_range = np.linspace( -2.0, 2.0, 500 )\n",
        "  y_range = np.linspace( -2.0, 2.0, 500 )\n",
        "  xx, yy = np.meshgrid( x_range, y_range )\n",
        "  fig, ax = plt.subplots()\n",
        "  dset = np.c_[xx.ravel(), yy.ravel()]\n",
        "  dset = torch.Tensor(dset).to( device)\n",
        "  Z = model(dset)\n",
        "  Z = (Z>0).float().reshape(xx.shape).cpu().detach().numpy()\n",
        "\n",
        "  ax.contourf(xx, yy, Z, cmap = plt.cm.Paired)\n",
        "  ax.scatter(dataset['raw'][:,0], dataset['raw'][:,1], c = dataset['raw'][:,2].astype( np.int ), cmap = plt.cm.autumn)\n",
        "  plt.title(title, fontsize=17)\n",
        "  plt.ylabel('y', fontsize=14)\n",
        "  plt.xlabel('x', fontsize=14)\n",
        "  plt.grid()\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XqRRf3Z2LVM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_classification_surface(model, data_train, 'Conjunto de Datos de Entrenamiento')\n",
        "plot_classification_surface(model, data_test, 'Conjunto de Datos de Test')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e91eKoCmWEWG",
        "colab_type": "text"
      },
      "source": [
        "### ¿ Qué pasa si agregamos regularización L2 ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5y_2F1HWFTg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "\n",
        "device = torch.device( 'cuda' if torch.cuda.is_available() else 'cpu' )\n",
        "\n",
        "# Creación de Dataloader\n",
        "two_classes_trainset = TwoClassesDataset(data_train['raw'], transform = transforms.Compose([ToTensor()]))\n",
        "dataloader_train = DataLoader(two_classes_trainset, batch_size = 256, shuffle = True, num_workers = 1)\n",
        "two_classes_testset = TwoClassesDataset(data_test['raw'], transform = transforms.Compose([ToTensor()]))\n",
        "dataloader_test = DataLoader(two_classes_testset, batch_size = 256, shuffle = False, num_workers = 1)\n",
        "\n",
        "# Creación de modelo\n",
        "model = ClassificationModel()\n",
        "model.to( device )\n",
        "\n",
        "# Creación de instancias para la función de pérdida y optimizador\n",
        "# Parámetro Weight Decay indica la importancia que asignaremos a la regularización L2 en la pérdida(en este caso, 0.2)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr = 1e-3, weight_decay = 0.2)\n",
        "\n",
        "# Entrenamiento del modelo\n",
        "fit(model, criterion, optimizer, dataloader_train, 150)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBrIgAT7t2Hp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "evaluate(model, criterion, dataloader_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkQ5Gy5AWKib",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_classification_surface(model, data_train, 'Conjunto de Datos de Entrenamiento')\n",
        "plot_classification_surface(model, data_test, 'Conjunto de Datos de Test')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKftFZ2W4XKJ",
        "colab_type": "text"
      },
      "source": [
        "### Actividades"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZP7f0Vp5uqO",
        "colab_type": "text"
      },
      "source": [
        "*    **1. Explique brevemente ¿Cuál es la principal diferencia entre la forma de los límites de clasificación?**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M92rfhgQu6Me",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Respuesta 1\n",
        "\n",
        "Respuesta = 'Escriba aquí su respuesta' #@param {type:\"string\"}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1aXRQUH5zfB",
        "colab_type": "text"
      },
      "source": [
        "*     **2. A nivel del modelo mismo, ¿qué explica esta diferencia por el hecho de agregar o quitar la regularización?**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOe4ZRtzvA1N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Respuesta 2\n",
        "\n",
        "Respuesta = 'Escriba aquí su respuesta' #@param {type:\"string\"}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lo0Ww-0Stg78",
        "colab_type": "text"
      },
      "source": [
        "*    **3. Al comparar el modelo sin regularización v/s el con regularización.**\n",
        "    * **¿Cuál es mejor en el conjunto de entrenamiento?**\n",
        "    * **¿Cuánto accuracy obtuvo en el conjunto de entrenamiento?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3-8g-4OvI65",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Respuesta 3\n",
        "\n",
        "Mejor Modelo = 'seleccione una opción' #@param [\"seleccione una opción\", \"Con Regularizacion\", \"Sin Regularización\", \"Son iguales\"]\n",
        "Accuracy = 'Escriba aquí el porcentaje de accuracy que consiguió su ejecución' #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhoZkgpLxG4a",
        "colab_type": "text"
      },
      "source": [
        "*    **4. Al comparar el modelo sin regularización v/s el con regularización.**\n",
        "    * **¿Cuál es mejor en el conjunto de test?**\n",
        "    * **¿Cuánto accuracy obtuvo en el conjunto de test?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFXaJsHkxMYE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Respuesta 4\n",
        "\n",
        "Mejor Modelo = 'seleccione una opción' #@param [\"seleccione una opción\", \"Con Regularizacion\", \"Sin Regularización\", \"Son iguales\"]\n",
        "Accuracy = 'Escriba aquí el porcentaje de accuracy que consiguió su ejecución' #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDdjwX69xZ-3",
        "colab_type": "text"
      },
      "source": [
        "*    **5. Unicamente considerando lo analizado en las preguntas 3 y 4. ¿Qué estrategia recomendaría usar para generar el modelo definitivo?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJQhbVAdx_AC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Respuesta 5\n",
        "\n",
        "Modelo seleccionado = 'seleccione una opción' #@param [\"seleccione una opción\", \"Con Regularizacion\", \"Sin Regularización\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnEhhS8VxTYd",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mncSpB6duQBz",
        "colab_type": "text"
      },
      "source": [
        "*    **6. Revisando el código anterior, ¿qué línea tendría que editar si usted quisiera aumentar a 0.9 la importancia de la regularización L2? Copie y pegue la línea de código modificada aquí**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avB9Sng5zAnm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Respuesta 6\n",
        "\n",
        "Código modificado = 'Pegue aquí la línea de código modificada' #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5CrhP8kEOnu",
        "colab_type": "text"
      },
      "source": [
        "*    **7. Ejecute el código con esta regularización L2 ponderada por 0.9. ¿Qué efecto tiene en la curva que separa las clases?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlFRQlhZZzp7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Respuesta 7\n",
        "\n",
        "Respuesta = 'Escriba muy brevemente lo observado' #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FVLGUPjBy9l",
        "colab_type": "text"
      },
      "source": [
        "## Experimento 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6s2K3JoU_bUP",
        "colab_type": "text"
      },
      "source": [
        "En este experimento usaremos un dataset llamado **celebA**, que consta de imágenes RGB de caras de celebridades. \n",
        "\n",
        "Este dataset cuenta con **40 anotaciones diferentes** (clase binaria) para cada una de las imágenes, además de una anotación en coordenadas (x,y) para 5 keypoints:\n",
        "*  **Landmarks**: posición (x,y) de 5 puntos clave (ojos, nariz y extremos de la boca)\n",
        "*  **Attractive**: Un valor binario que indica si el anotador consideraba que la persona era atractiva o no\n",
        "*  **Mustache**: Un valor binario que indica si la persona tiene bigote o no\n",
        "*  **Straight_Hair**: Un valor binario que indica si la persona tiene pelo liso o no\n",
        "*  etc\n",
        "\n",
        "Aqui dejamos un listado de todas las posibles tareas principales y todas las posibles tareas auxiliares. Esto nos ayudará a probar diferentes escenarios y observar cómo se comporta el modelo al combinar las tareas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EV4rnG2-oTT6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "primary_tasks = ['5_o_Clock_Shadow','Arched_Eyebrows','Attractive','Bags_Under_Eyes',\n",
        "                 'Bald','Bangs','Big_Lips','Big_Nose','Black_Hair','Blond_Hair',\n",
        "                 'Blurry','Brown_Hair','Bushy_Eyebrows','Chubby','Double_Chin',\n",
        "                 'Eyeglasses','Goatee','Gray_Hair','Heavy_Makeup','High_Cheekbones',\n",
        "                 'Male','Mouth_Slightly_Open','Mustache','Narrow_Eyes','No_Beard',\n",
        "                 'Oval_Face','Pale_Skin','Pointy_Nose','Receding_Hairline',\n",
        "                 'Rosy_Cheeks','Sideburns','Smiling','Straight_Hair','Wavy_Hair',\n",
        "                 'Wearing_Earrings','Wearing_Hat','Wearing_Lipstick','Wearing_Necklace',\n",
        "                 'Wearing_Necktie','Young']\n",
        "\n",
        "auxiliary_tasks = ['Landmarks','5_o_Clock_Shadow','Arched_Eyebrows','Attractive',\n",
        "                   'Bags_Under_Eyes','Bald','Bangs','Big_Lips','Big_Nose','Black_Hair',\n",
        "                   'Blond_Hair','Blurry','Brown_Hair','Bushy_Eyebrows','Chubby',\n",
        "                   'Double_Chin','Eyeglasses','Goatee','Gray_Hair','Heavy_Makeup',\n",
        "                   'High_Cheekbones','Male','Mouth_Slightly_Open','Mustache',\n",
        "                   'Narrow_Eyes','No_Beard','Oval_Face','Pale_Skin','Pointy_Nose',\n",
        "                   'Receding_Hairline','Rosy_Cheeks','Sideburns','Smiling','Straight_Hair',\n",
        "                   'Wavy_Hair','Wearing_Earrings','Wearing_Hat','Wearing_Lipstick',\n",
        "                   'Wearing_Necklace','Wearing_Necktie','Young']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsoCCEhwRujv",
        "colab_type": "text"
      },
      "source": [
        "Para hacer la tarea más difícil y para acelerar los experimentos, se pre-filtró las imágenes para quedarnos con 10.000 en cada set (entrenamiento, validación y test), y se transformaron a tamaño 40x40 pixeles.\n",
        "\n",
        "A continuación descargamos y descomprimimos el dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eq_hC8xop6j-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!if [ ! -f celebA.zip ]; then wget -q https://www.dropbox.com/s/xs6crlhugaufq7t/celebA.zip; fi\n",
        "!unzip -oq celebA.zip > /dev/null"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5pkmyq7H-Qo",
        "colab_type": "text"
      },
      "source": [
        "Y luego visualizaremos dos imágenes del dataset, junto con su imagen transformada y con los Landmarks graficados."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uc3Dln62Yqs7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import IPython \n",
        "pil_img = IPython.display.Image(filename='example.png')\n",
        "display(pil_img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aRaP9FAHvFk",
        "colab_type": "text"
      },
      "source": [
        "Aquí implementamos el dataloader y las funcionalidades para manejar los datos de entrada"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Vgakx1Azw3e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "import sklearn.metrics\n",
        "import time\n",
        "from IPython.display import clear_output\n",
        "import matplotlib\n",
        "import matplotlib.cm as cm\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "class FaceDataset(Dataset):\n",
        "\n",
        "    def __init__(self, annotation_path, images_dir, primary_task, auxiliary_task=None):\n",
        "        # read csv and store relevant data\n",
        "        annotations = pd.read_csv(annotation_path, sep=';') \n",
        "        self.images_dir = images_dir\n",
        "        self.primary_task = primary_task\n",
        "        self.auxiliary_task = auxiliary_task\n",
        "        self.transform = transforms.Compose([transforms.ToTensor()])\n",
        "        self.filenames = list(annotations['filename'])\n",
        "        self.primary_label = list(annotations[primary_task])\n",
        "        if auxiliary_task == 'Landmarks':\n",
        "            self.auxiliary_label = annotations.iloc[:,1:11]\n",
        "        elif auxiliary_task is not None:\n",
        "            self.auxiliary_label = list(annotations[auxiliary_task])\n",
        "        else:\n",
        "            self.auxiliary_label = None\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.filenames)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # read image file and apply transformation\n",
        "        img_path = os.path.join(self.images_dir, self.filenames[idx])\n",
        "        img_orig = Image.open(img_path)\n",
        "        img = self.transform(img_orig)\n",
        "\n",
        "        main_label = torch.tensor([float(self.primary_label[idx])]).float()\n",
        "\n",
        "        if self.auxiliary_task == 'Landmarks':\n",
        "            secondary_label = torch.tensor(np.array(self.auxiliary_label.iloc[idx,:]).astype('float')).float()\n",
        "        elif self.auxiliary_task is not None:\n",
        "            secondary_label = torch.tensor([float(self.auxiliary_label[idx])]).float()\n",
        "        else:\n",
        "            return { 'image': img, 'main_label': main_label }\n",
        "\n",
        "        return { 'image': img, 'main_label': main_label, 'auxiliary_label': secondary_label }\n",
        "\n",
        "\n",
        "def collate_imgs(batch):\n",
        "    # custom function to create the batches in the format that we want\n",
        "\n",
        "    # append elements to the corresponding entry of this new dictionary\n",
        "    batch_dict = {}\n",
        "    for item in batch:\n",
        "        for key in item:\n",
        "            if key not in batch_dict:\n",
        "                batch_dict[key] = [item[key].unsqueeze(0)]\n",
        "            else:\n",
        "                batch_dict[key].append(item[key].unsqueeze(0))\n",
        "\n",
        "    # convert into tensors\n",
        "    for key in batch_dict.keys():\n",
        "        batch_dict[key] = torch.cat(batch_dict[key])\n",
        "\n",
        "    return batch_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vOWcRTMGw4n",
        "colab_type": "text"
      },
      "source": [
        "Y luego creamos nuestro modelo.\n",
        "Intencionalmente usamos una arquitectura casi idéntica a la del Experimento 1. La principal diferencia es que incorporamos opcionalmente una predicción auxiliar que puede ser binaria o de 10 dimensiones, según la tarea."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUMfc_IOHQfw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!if [ ! -f modelo3.png ]; then wget -q https://www.dropbox.com/s/49se5glxq37lgi5/modelo3.png; fi\n",
        "import IPython \n",
        "pil_img = IPython.display.Image(filename='modelo3.png')\n",
        "display(pil_img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXZ5AKuk8eID",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class FaceModel(nn.Module):\n",
        "    def __init__(self, auxiliary_task_dim=10):\n",
        "        super(FaceModel, self).__init__()\n",
        "        self.auxiliary_task_dim = auxiliary_task_dim\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 7 * 7, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 1)\n",
        "        if auxiliary_task_dim is not None:\n",
        "            self.fc4 = nn.Linear(84, auxiliary_task_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 7 * 7)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        main_task = self.fc3(x)\n",
        "        if self.auxiliary_task_dim is not None:\n",
        "            auxiliary_task = self.fc4(x)\n",
        "            return main_task, auxiliary_task\n",
        "        else:\n",
        "            return main_task, None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59XRsSpOIZ9z",
        "colab_type": "text"
      },
      "source": [
        "En la siguiente celda implementamos funcionalidades para calcular métricas y el loop de entrenamiento / test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wvl1yOdyJTbK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_landmarks_distances(real, pred):\n",
        "    # compute element-wise difference (xi_pred - xi_real)\n",
        "    diff = real - pred\n",
        "\n",
        "    # compute element-wise square (xi_pred - xi_real)**2\n",
        "    squared_diff = torch.mul(diff, diff)\n",
        "\n",
        "    # reshape to 2 columns, so that each (xi, yi) are in one row\n",
        "    pairs = squared_diff.reshape(-1, 2)\n",
        "\n",
        "    # compute sqrt((xi_pred - xi_real)**2 + (yi_pred - yi_real)**2)\n",
        "    distances = torch.sqrt(torch.sum(pairs,1))\n",
        "\n",
        "    # replace nan by zero generated by sqrt(negative) due to floating point imprecisions \n",
        "    distances[torch.isnan(distances)] = 0.0\n",
        "\n",
        "    return distances\n",
        "\n",
        "\n",
        "def compute_corrects(real, pred):\n",
        "    # compute final predictions. Remember that we ommited the sigmoid, so \n",
        "    # positive values will be one, as sigmoid(positive_value) is closer to 1 than to 0. \n",
        "    # Negative values will be zero, as sigmoid(negative_value) is closer to 0 than to 1. \n",
        "    final_preds = (pred > 0.0).float()\n",
        "\n",
        "    # compute element-wise absolute difference (xi_pred - xi_real)\n",
        "    diff = torch.abs(real.float() - final_preds)\n",
        "\n",
        "    # element is correct if the difference is lower than 0.5\n",
        "    corrects = (diff < 0.5).float()\n",
        "\n",
        "    return corrects\n",
        "\n",
        "\n",
        "def train_loop(model, train_dataloader, val_dataloader, optimizer, criterion, epochs, device):\n",
        "\n",
        "    # train loop\n",
        "    best_val_acc = -1.0\n",
        "    best_epoch = -1\n",
        "    best_weights = None\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        print('\\nTRAINING EPOCH {} of {}'.format(epoch+1, epochs))\n",
        "\n",
        "        # TRAIN\n",
        "        model.train()\n",
        "        for item in train_dataloader:\n",
        "            # send image and labels to the device\n",
        "            image = item['image'].to(device)\n",
        "\n",
        "            # create a list of all labels. The first one corresponds to the primary task\n",
        "            main_labels = item['main_label'].to(device)\n",
        "            auxiliary_labels = None\n",
        "            if 'auxiliary_label' in item:\n",
        "                auxiliary_labels = item['auxiliary_label'].to(device)\n",
        "            \n",
        "            # clear gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward pass\n",
        "            main_predictions, auxiliary_predictions = model(image)\n",
        "\n",
        "            # compute loss\n",
        "            loss = criterion(main_predictions, auxiliary_predictions, main_labels, auxiliary_labels)\n",
        "\n",
        "            # backward pass\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "\n",
        "        # EVALUATE IN DEV SET\n",
        "        model.eval()\n",
        "        item_count = 0\n",
        "        running_metrics = { 'loss': 0.0, 'primary_task_acc': 0.0}\n",
        "        with torch.set_grad_enabled(False):\n",
        "            for item in val_dataloader:\n",
        "                # send image and labels to the device\n",
        "                image = item['image'].to(device)\n",
        "\n",
        "                # create a list of all labels. The first one corresponds to the primary task\n",
        "                main_labels = item['main_label'].to(device)\n",
        "                auxiliary_labels = None\n",
        "                if 'auxiliary_label' in item:\n",
        "                    auxiliary_labels = item['auxiliary_label'].to(device)\n",
        "                \n",
        "                # forward pass\n",
        "                main_predictions, auxiliary_predictions = model(image)\n",
        "\n",
        "                # compute loss\n",
        "                loss = criterion(main_predictions, auxiliary_predictions, main_labels, auxiliary_labels)\n",
        "                \n",
        "                # update statistics\n",
        "                batch_sz = image.size(0)    \n",
        "                item_count += batch_sz\n",
        "                running_metrics['loss'] += loss.item() * float(batch_sz)\n",
        "                \n",
        "                # count number of correctly classified samples \n",
        "                # (later on we will divide by the total)\n",
        "                corrects = compute_corrects(main_labels, main_predictions)\n",
        "                running_metrics['primary_task_acc'] += torch.sum(corrects)\n",
        "\n",
        "            # divide by total, to get the average, and print statistics\n",
        "            epoch_metrics = { key: float(running_metrics[key]) / float(item_count) for key in running_metrics } \n",
        "            print('   Dev set statistics')\n",
        "            print('     loss:        {0:.15f}'.format(epoch_metrics['loss']))\n",
        "            print('     accuracy:    {0:.15f}'.format(epoch_metrics['primary_task_acc']))\n",
        "\n",
        "            # store best model if needed\n",
        "            if epoch_metrics['primary_task_acc'] > best_val_acc:\n",
        "                best_val_acc = epoch_metrics['primary_task_acc']\n",
        "                best_epoch = epoch\n",
        "                best_weights = model.state_dict()\n",
        "            print('     Best Accuracy:   {0:.5f}'.format(best_val_acc))\n",
        "\n",
        "    return { 'best_acc': best_val_acc, \n",
        "             'best_epoch': best_epoch, \n",
        "             'best_weights': best_weights }\n",
        "\n",
        "    return result\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAOnaeYaIjh4",
        "colab_type": "text"
      },
      "source": [
        "En esta clase implementaremos nuestra función de pérdida, que permite opcionalmente incorporar una pérdida correspondiente a la tarea auxiliar.\n",
        "\n",
        "En caso de que la tarea auxiliar sea binaria, utilizaremos Cross-Entropy (binaria), pero si es la tarea de 'Landmarks', usaremos MSE, pues queremos que el modelo maneje la predicción de las coordenadas (x,y) como un problema de regresión.\n",
        "\n",
        "La tarea principal, por simplicidad, asumiremos que siempre es binaria (y por lo tanto usaremos Cross-Entropy.\n",
        "\n",
        "Recordemos lo que vimos en clases sobre cómo combinar ambas pérdidas:\n",
        "\n",
        "$$ \\mathit{Loss} = \\mathit{Loss}_{principal} + \\lambda \\mathit{Loss}_{auxiliar} $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKHjM6Q9O9Sg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CombinedLoss(nn.Module):\n",
        "\n",
        "    def __init__(self, auxiliary_task, auxiliary_weight):\n",
        "        super(CombinedLoss, self).__init__()\n",
        "\n",
        "        # store relevant information\n",
        "        self.auxiliary_task = auxiliary_task\n",
        "        self.aux_weight = auxiliary_weight # this is the weight \"lambda\" to combine both loss functions\n",
        "        \n",
        "    def forward(self, main_predictions, auxiliary_predictions, main_labels, auxiliary_labels):\n",
        "\n",
        "        if auxiliary_labels is None:\n",
        "            # if we are not using auxiliary task, just return a cross-entropy\n",
        "            return F.binary_cross_entropy_with_logits(main_predictions, main_labels)\n",
        "        else:\n",
        "            # if we are using auxiliary task, return the main cross-entropy and the auxiliary loss, combined\n",
        "            main_loss = F.binary_cross_entropy_with_logits(main_predictions, main_labels)\n",
        "            if self.auxiliary_task == 'Landmarks':\n",
        "                aux_loss = F.mse_loss(auxiliary_predictions, auxiliary_labels)\n",
        "            else:\n",
        "                aux_loss = F.binary_cross_entropy_with_logits(auxiliary_predictions, auxiliary_labels)\n",
        "\n",
        "            # return main_loss + lambda * auxiliary_loss\n",
        "            return main_loss + self.aux_weight * aux_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xp6D2x2zKlkv",
        "colab_type": "text"
      },
      "source": [
        "A continuación está el código para lanzar el entrenamiento. En este primer ejemplo, usaremos como tarea principal el atributo 'Attractive' sin tarea auxiliar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUW2eWM2Ym4D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "images_path = 'celebA'\n",
        "batch_size = 64\n",
        "epochs = 20\n",
        "learning_rate = 0.001\n",
        "primary_task = 'Attractive'\n",
        "auxiliary_task = None\n",
        "auxiliary_weight = None\n",
        "weight_decay = 0.0\n",
        "\n",
        "train_dataset = FaceDataset('annotations_train.csv', images_path, primary_task, auxiliary_task=auxiliary_task)\n",
        "val_dataset = FaceDataset('annotations_val.csv', images_path, primary_task, auxiliary_task=auxiliary_task)\n",
        "test_dataset = FaceDataset('annotations_test.csv', images_path, primary_task, auxiliary_task=auxiliary_task)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=3, collate_fn=collate_imgs)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=3, collate_fn=collate_imgs)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=3, collate_fn=collate_imgs)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "if auxiliary_task is None:\n",
        "    model = FaceModel(None).to(device)\n",
        "elif auxiliary_task == 'Landmarks':\n",
        "    model = FaceModel(10).to(device)\n",
        "else:\n",
        "    model = FaceModel(1).to(device)\n",
        "\n",
        "criterion = CombinedLoss(auxiliary_task, auxiliary_weight)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "training_result = train_loop(model, train_dataloader, val_dataloader, optimizer, criterion, epochs, device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtf4jnn-QCEq",
        "colab_type": "text"
      },
      "source": [
        "Ahora probaremos el mismo modelo, pero usando una tarea auxiliar binaria (\"Young\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ckNP49gW5oZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "images_path = 'celebA'\n",
        "batch_size = 64\n",
        "epochs = 20\n",
        "learning_rate = 0.001\n",
        "primary_task = 'Attractive'\n",
        "auxiliary_task = 'Young'\n",
        "auxiliary_weight = 0.1\n",
        "weight_decay = 0.0\n",
        "\n",
        "train_dataset = FaceDataset('annotations_train.csv', images_path, primary_task, auxiliary_task=auxiliary_task)\n",
        "val_dataset = FaceDataset('annotations_val.csv', images_path, primary_task, auxiliary_task=auxiliary_task)\n",
        "test_dataset = FaceDataset('annotations_test.csv', images_path, primary_task, auxiliary_task=auxiliary_task)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=3, collate_fn=collate_imgs)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=3, collate_fn=collate_imgs)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=3, collate_fn=collate_imgs)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "if auxiliary_task is None:\n",
        "    model = FaceModel(None).to(device)\n",
        "elif auxiliary_task == 'Landmarks':\n",
        "    model = FaceModel(10).to(device)\n",
        "else:\n",
        "    model = FaceModel(1).to(device)\n",
        "\n",
        "criterion = CombinedLoss(auxiliary_task, auxiliary_weight)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "training_result = train_loop(model, train_dataloader, val_dataloader, optimizer, criterion, epochs, device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGsnxjmLQQVr",
        "colab_type": "text"
      },
      "source": [
        "Finalmente probaremos una tarea principal de clasificación binaria (\"Young\") con una tarea auxiliar de regresión (\"Landmarks\")."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zG0dvHSgb94v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "images_path = 'celebA'\n",
        "batch_size = 64\n",
        "epochs = 20\n",
        "learning_rate = 0.001\n",
        "primary_task = 'Young'\n",
        "auxiliary_task = 'Landmarks'\n",
        "auxiliary_weight = 0.1\n",
        "weight_decay = 0.0\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "train_dataset = FaceDataset('annotations_train.csv', images_path, primary_task, auxiliary_task=auxiliary_task)\n",
        "val_dataset = FaceDataset('annotations_val.csv', images_path, primary_task, auxiliary_task=auxiliary_task)\n",
        "test_dataset = FaceDataset('annotations_test.csv', images_path, primary_task, auxiliary_task=auxiliary_task)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=3, collate_fn=collate_imgs)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=3, collate_fn=collate_imgs)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=3, collate_fn=collate_imgs)\n",
        "\n",
        "if auxiliary_task is None:\n",
        "    model = FaceModel(None).to(device)\n",
        "elif auxiliary_task == 'Landmarks':\n",
        "    model = FaceModel(10).to(device)\n",
        "else:\n",
        "    model = FaceModel(1).to(device)\n",
        "\n",
        "criterion = CombinedLoss(auxiliary_task, auxiliary_weight)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "training_result = train_loop(model, train_dataloader, val_dataloader, optimizer, criterion, epochs, device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csAEszF6CHdT",
        "colab_type": "text"
      },
      "source": [
        "## BONUS Opcional: 5 ganadores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndwtD2w-CK5u",
        "colab_type": "text"
      },
      "source": [
        "**Los 5 alumnos (o parejas) con mejor score serán premiados con un bonus de 5 décimas en la tarea (nota máxima 7.0, no acumulable a otras tareas).** \n",
        "\n",
        "**Utilizando sus conocimientos aprendidos hasta el momento, pruebe diferentes configuraciones para el modelo e hiperparámetros de entrenamiento. El objetivo es buscar dos tareas tales que el modelo con tarea auxiliar sea superior al sin tarea auxiliar.**\n",
        "\n",
        "**Restricciones:** \n",
        "*  El modelo sin tarea auxiliar y el con tarea auxiliar deben ser corridos exactamente con los mismos hiperpárametros, excepto los que respectan a la tarea auxiliar (auxiliary_task y auxiliary_weight). \n",
        "*  No se permite modificar el código principal, solamente puede variar los siguientes hiperparámetros:\n",
        "  *  Número de épocas\n",
        "  *  Tamaño del Batch\n",
        "  *  Learning Rate\n",
        "  *  primary_task\n",
        "  *  auxiliary_task\n",
        "  *  auxiliary_weight\n",
        "  *  weight_decay\n",
        "* Deberá reportar el accuracy de validación de ambos modelos (\"Best Accuracy\") y la diferencia de ambos (que será el score que decidirá a los ganadores)\n",
        "* Las celdas de ejecución los dos experimentos reportados deben estar ejecutadas y visibles para su verificación. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zRhf5AxFMdj",
        "colab_type": "text"
      },
      "source": [
        "**Escriba aquí su respuesta:** (Haga doble click para editar esta celda)\n",
        "\n",
        "*  **Número de épocas:** ... ej: 15\n",
        "*  **Tamaño del Batch:** ... ej: 64\n",
        "*  **Learning Rate:** ... ej: 0.001\n",
        "*  **primary_task:** ... ej: 'Attractive'\n",
        "*  **auxiliary_task:** ... ej: 'Landmarks'\n",
        "*  **auxiliary_weight:** ... ej: 0.1\n",
        "*  **weight_decay:** ... ej: 0.0\n",
        "*  **Best Accuracy sin tarea auxiliar:** ... ej: 75.2%\n",
        "*  **Best Accuracy con tarea auxiliar:** ... ej: 75.3%\n",
        "*  **Diferencia entre ambos accuracies:** ... ej: 0.1%  (75.3% - 75.2%)"
      ]
    }
  ]
}